---
title: "Lecture 4: Model checking"
author: "David L Miller (`@millerdl`)"
output:
  xaringan::moon_reader:
    nature:
      highlightStyle: github
      highlightLines: true
    seal: false
    lib_dir: libs
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_HTMLorMML"
css: custom.css

---

```{r setup, include=FALSE}
# setup
library(knitr)
library(magrittr)
library(viridis)
library(reshape2)
library(animation)
opts_chunk$set(cache=TRUE, echo=FALSE, warning=FALSE, error=FALSE,
               message=FALSE, fig.height=8, fig.width=10)

# some useful libraries
library(RColorBrewer)
library(ggplot2)
library(cowplot)
theme_set(theme_cowplot(20))

```

class: title-slide, inverse, center, middle

# Lecture 4: Model checking
<div style="position: absolute; bottom: 15px; vertical-align: center; left: 10px">
<img src="images/02-foundation-vertical-white.png" height="200">
</div>

---




```{r initialmodeletc, echo=FALSE, message=FALSE, warning=FALSE}
load("../data/spermwhale.RData")
library(Distance)
library(dsm)
df <- ds(distdata, truncation=6000)

obs <- obs[obs$distance <= 6000, ]

dsm_tw_xy_depth <- dsm(count ~ s(x, y) + s(Depth), ddf.obj=df, observation.data=obs, segment.data=segs, family=tw())
dsm_x_tw <- dsm(count~s(x), ddf.obj=df,
                segment.data=segs, observation.data=obs,
                family=tw())
```

class: inverse, middle, center


## *"perhaps the most important part of applied statistical modelling"*

### Simon Wood

---
# Model checking

- As with detection functions, checking is important
- Checking *doesn't* mean your model is **right**
- Want to know the model conforms to assumptions
- What assumptions should we check?

---
class: inverse, middle, center
# Convergence

---
# Convergence

- Fitting the GAM involves an optimization
- By default this is REstricted Maximum Likelihood (REML) score
- Sometimes this can go wrong
- R will warn you!

---
# A model that converges

```{r convcheck, echo=TRUE, fig.keep="none"}
gam.check(dsm_tw_xy_depth)
```

---
# A bad model

```
Error in while (mean(ldxx/(ldxx + ldss)) > 0.4) { :
  missing value where TRUE/FALSE needed
In addition: Warning message:
In sqrt(w) : NaNs produced
Error in while (mean(ldxx/(ldxx + ldss)) > 0.4) { :
  missing value where TRUE/FALSE needed
```

This is **rare**

---
class: inverse, middle, center
# The Folk Theorem of Statistical Computing

## *"most statistical computational problems are due not to the algorithm being used but rather the model itself"*

### Andrew Gelman 

---
# Folk Theorem anecdata

- Often if there are fitting problems, you're asking too much from your data
- Model is too complicated
- Too little data
- Try something simpler, see what happens


---
class: inverse, middle, center
# Basis size

---
# Basis size (k)

- Set `k` per term
- e.g. `s(x, k=10)` or `s(x, y, k=100)`
- Penalty removes "extra" wigglyness
  - *up to a point!*
- (But computation is slower with bigger `k`)

---
# Checking basis size

```{r gamcheck-text, fig.keep="none", echo=TRUE}
gam.check(dsm_x_tw)
```

---
# Increasing basis size

```{r gamcheck-kplus-text, fig.keep="none", echo=TRUE}
dsm_x_tw_k <- dsm(count~s(x, k=20), ddf.obj=df,
                  segment.data=segs, observation.data=obs,
                  family=tw())
gam.check(dsm_x_tw_k)
```

---
# Sometimes basis size isn't the issue...

- Generally, double `k` and see what happens
- Didn't increase the EDF much here
- Other things can cause low "`p-value`" and "`k-index`"
- Increasing `k` can cause problems

---
# k is a maximum

- Don't worry about things being too wiggly
- `k` gives the maximum complexity
- Penalty deals with the rest

```{r plotk, fig.width=18, fig.height=9 }
dsm_sst_k5 <- dsm(count~s(SST, k=5), ddf.obj=df,
                  segment.data=segs, observation.data=obs,
                  family=tw())
dsm_sst_k20 <- dsm(count~s(SST, k=20), ddf.obj=df,
                  segment.data=segs, observation.data=obs,
                  family=tw())
dsm_sst_k50 <- dsm(count~s(SST, k=50), ddf.obj=df,
                  segment.data=segs, observation.data=obs,
                  family=tw())
par(mfrow=c(1,3), cex.lab=2, lwd=3, cex.main=4, cex.axis=2, cex.lab=4, mar=c(5,6.5,4,2)+0.1)
plot(dsm_sst_k5, main="k=5")
plot(dsm_sst_k20, main="k=20")
plot(dsm_sst_k50, main="k=50")
```

---
class: inverse, middle, center
# Residuals

---
# What are residuals?

- Generally residuals = observed value - fitted value
- BUT hard to see patterns in these "raw" residuals
- Need to standardise $\Rightarrow$ **deviance residuals**
- Expect these residuals $\sim N(0,1)$

---
# Why are residuals important?

- Structure in the residuals means your model didn't capture something
- Maybe a missing covariate
- Model doesn't describe the data well

---
# Residuals vs. covariates

```{r covar-resids, fig.width=15}
par(cex.axis=2, lwd=2, cex.lab=2)
library(statmod)
par(mfrow=c(1,2))
plot(dsm_x_tw$data$x, residuals(dsm_x_tw), ylab="Deviance residuals", xlab="x")
environment(dsm_x_tw$family$variance)$p <- dsm_x_tw$family$getTheta(TRUE)
plot(dsm_x_tw$data$x, qres.tweedie(dsm_x_tw), ylab="Randomised quantile residuals", xlab="x")
```

---
# Residuals vs. covariates (boxplots)

```{r covar-resids-boxplot, fig.width=15}
par(cex.axis=2, lwd=2, cex.lab=2, las=2)
library(statmod)
par(mfrow=c(1,2))

resid_dat <- data.frame(x     = dsm_x_tw$data$x,
                        x_cut = cut(dsm_x_tw$data$x,
                                    seq(min(dsm_x_tw$data$x),
                                        max(dsm_x_tw$data$x),
                                        len=20)),
                        dres  = residuals(dsm_x_tw),
                        qres  = qres.tweedie(dsm_x_tw))

plot(dres~x_cut, data=resid_dat, ylab="Deviance residuals", xlab="x")
plot(qres~x_cut, data=resid_dat, ylab="Randomised quantile residuals", xlab="x")
```

---
# Fitting to residuals

- Refit our model but with the residuals as response
- Response is normal (for deviance residuals)
- What pattern is left in the residuals?

---
# Example

```{r resid-refit-firstfit, echo=FALSE}
df_hr <- ds(distdata, truncation=6000, key="hr")
dsm_depth_npp <- dsm(count~ s(Depth, bs="ts", k=20) +
                        s(NPP, bs="ts", k=20),
                  ddf.obj=df_hr,
                  segment.data=segs, observation.data=obs,
                  family=tw())
```

- Example model with `NPP` and `Depth`

```{r resid-refit, echo=TRUE}
# get data
refit_dat <- dsm_depth_npp$data
# make residuals column
refit_dat$resid <- residuals(dsm_depth_npp)
# fit a model (same model)
resid_fit <- gam(resid~s(Depth, bs="ts", k=20) +
                        s(NPP, bs="ts", k=20),
                 family=gaussian(), data=refit_dat, method="REML")
```

---
# `summary(resid_fit)`

```{r resid-refit-summary}
summary(resid_fit)
```

---
# What's going on there?

- Something unexplained going on?
- Maybe `Depth` + `NPP` is not enough?
  - Add other smooths (`s(x, y)`? )
- Increase `k`?

---
class: inverse, middle, center
# Other residual checking

---
# `gam.check` 

```{r gamcheck, results="hide"}
gam.check(dsm_x_tw)
```

---
# Shortcomings

- `gam.check` can be helpful
- "Resids vs. linear pred" is victim of artifacts
- Need an alternative
- "Randomised quanitle residuals"
  - `rqgam.check`
  - Exactly normal residuals

---
# Randomised quantile residuals

```{r rqgamcheck}
rqgam.check(dsm_x_tw)
```

---
# Example of "bad" plots

![Bad residual check plot from Wood 2006](images/badgam.png)

---
# Example of "bad" plots

![Bad residual check plot from Wood 2006](images/badgam-annotate.png)

---
# Residual checks

- Looking for patterns (not artifacts)
- This can be tricky
- Need to use a mixture of techniques
- Cycle through checks, make changes recheck

---
class: inverse, middle, center
# Observed vs. expected


---
# Response vs. fitted values

- `gam.check` "response vs. fitted values"
- BUT smooths are "wrong" everywhere in particular

```{r gamcheck-response-fitted, fig.height=6, fig.width=6}
class(dsm_x_tw) <- class(dsm_x_tw)[-1]
fv <- predict(dsm_x_tw, type = "response")
plot(fv, dsm_x_tw$y, xlab = "Fitted Values", 
     ylab = "Response", main = "Response vs. Fitted Values")
```

---
# Summarize over covariate chunks

- On average the smooth is right
- Check aggregations of count
- Here detection function has Beaufort as factor

```{r df-beau-model}
distdata$Beaufort_f <- cut(distdata$Beaufort, 0:5, include.lowest=TRUE)
segs$Beaufort_f <- cut(segs$Beaufort, 0:5, include.lowest=TRUE)
df_bf <- ds(distdata, truncation=6000, formula=~Beaufort_f)

# bad
dsm_bad <- dsm(count ~ y, ddf.obj=df_bf, observation.data=obs, segment.data=segs, family=nb())

# good
dsm_good <- dsm(count~s(x,y)+s(Depth), ddf.obj=df_bf,
                segment.data=segs, observation.data=obs,
                family=tw())
```

```{r oe-beau, echo=TRUE}
obs_exp(dsm_bad, "Beaufort_f")
```

```{r oe-beau1, echo=TRUE}
obs_exp(dsm_good, "Beaufort_f")
```

---
# Observed vs. expected for environmental covariates

- Just need to specify the cutpoints

```{r oe-depth-bad, echo=TRUE}
obs_exp(dsm_bad, "Depth", c(0, 1000, 2000, 3000, 4000, 6000))
```


```{r oe-depth, echo=TRUE}
obs_exp(dsm_good, "Depth", c(0, 1000, 2000, 3000, 4000, 6000))
```

---
# Summary

- Convergence
  - Rarely an issue
- Basis size
  - `k` is a maximum
  - Double and see what happens
- Residuals
  - Deviance and randomised quantile
  - check for artifacts
- Observed vs. expected
  - Compare aggregate information
